{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"iris.csv\", index_col=None, header=None)\n",
    "df.columns = ['x1', 'x2', 'x3', 'x4', 'y']\n",
    "df = df.iloc[50:150]\n",
    "df['y'] = df['y'].apply(lambda x: 0 if x=='Iris-versicolor' else 1)\n",
    "\n",
    "X = torch.tensor(df[['x2', 'x3']].values, dtype=torch.float)\n",
    "y = torch.tensor(df['y'].values, dtype=torch.int)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n",
    "\n",
    "X, y = X[shuffle_idx], y[shuffle_idx]\n",
    "\n",
    "percent70 = int(shuffle_idx.size(0)*0.7)\n",
    "\n",
    "X_train, X_test = X[shuffle_idx[:percent70]], X[shuffle_idx[percent70:]]\n",
    "y_train, y_test = y[shuffle_idx[:percent70]], y[shuffle_idx[percent70:]]\n",
    "\n",
    "# Normalize (mean zero, unit variance)\n",
    "\n",
    "mu, sigma = X_train.mean(dim=0), X_train.std(dim=0)\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline2():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weight = torch.zeros(num_features, 1, \n",
    "                                  dtype=torch.float,\n",
    "                                  requires_grad=True)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float,\n",
    "                                requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        netinputs = torch.add(torch.mm(x, self.weight), self.bias)\n",
    "        activations = netinputs\n",
    "        return activations.view(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(yhat, y):\n",
    "    return torch.mean((yhat - y)**2)\n",
    "\n",
    "def train(model, x, y, num_epochs,\n",
    "          learning_rate=0.01, seed=123, minibatch_size=10):\n",
    "    cost = []\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        #### Shuffle epoch\n",
    "        shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n",
    "        minibatches = torch.split(shuffle_idx, minibatch_size)\n",
    "        \n",
    "        for minibatch_idx in minibatches:\n",
    "\n",
    "            #### Compute outputs ####\n",
    "            yhat = model.forward(x[minibatch_idx])\n",
    "            loss = loss_func(yhat, y[minibatch_idx])\n",
    "            \n",
    "            #### Compute gradients ####\n",
    "            \n",
    "            negative_grad_w = grad(loss, model.weight, retain_graph=True)[0] * (-1)\n",
    "            negative_grad_b = grad(loss, model.bias)[0] * (-1)\n",
    "            \n",
    "            \n",
    "            #### Update weights ####\n",
    "\n",
    "            model.weight = model.weight + learning_rate * negative_grad_w\n",
    "            model.bias = model.bias + learning_rate * negative_grad_b\n",
    "\n",
    "        #### Logging ####\n",
    "        with torch.no_grad():\n",
    "            # context manager to\n",
    "            # avoid building graph during \"inference\"\n",
    "            # to save memory\n",
    "            yhat = model.forward(x)\n",
    "            curr_loss = loss_func(yhat, y)\n",
    "            print('Epoch: %03d' % (e+1), end=\"\")\n",
    "            print(' | MSE: %.5f' % curr_loss)\n",
    "            cost.append(curr_loss)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | MSE: 0.38849\n",
      "Epoch: 002 | MSE: 0.31679\n",
      "Epoch: 003 | MSE: 0.26379\n",
      "Epoch: 004 | MSE: 0.22463\n",
      "Epoch: 005 | MSE: 0.19527\n",
      "Epoch: 006 | MSE: 0.17307\n",
      "Epoch: 007 | MSE: 0.15629\n",
      "Epoch: 008 | MSE: 0.14352\n",
      "Epoch: 009 | MSE: 0.13360\n",
      "Epoch: 010 | MSE: 0.12600\n",
      "Epoch: 011 | MSE: 0.12007\n",
      "Epoch: 012 | MSE: 0.11547\n",
      "Epoch: 013 | MSE: 0.11178\n",
      "Epoch: 014 | MSE: 0.10884\n",
      "Epoch: 015 | MSE: 0.10656\n",
      "Epoch: 016 | MSE: 0.10470\n",
      "Epoch: 017 | MSE: 0.10320\n",
      "Epoch: 018 | MSE: 0.10200\n",
      "Epoch: 019 | MSE: 0.10105\n",
      "Epoch: 020 | MSE: 0.10025\n"
     ]
    }
   ],
   "source": [
    "model = Adaline2(num_features=X_train.size(1))\n",
    "cost = train(model, \n",
    "             X_train, y_train.float(),\n",
    "             num_epochs=20,\n",
    "             learning_rate=0.01,\n",
    "             seed=123,\n",
    "             minibatch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 92.86\n",
      "Test Accuracy: 93.33\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(y_train.size())\n",
    "zeros = torch.zeros(y_train.size())\n",
    "train_pred = model.forward(X_train)\n",
    "train_acc = torch.mean(\n",
    "    (torch.where(train_pred > 0.5, \n",
    "                 ones, \n",
    "                 zeros).int() == y_train).float())\n",
    "\n",
    "ones = torch.ones(y_test.size())\n",
    "zeros = torch.zeros(y_test.size())\n",
    "test_pred = model.forward(X_test)\n",
    "test_acc = torch.mean(\n",
    "    (torch.where(test_pred > 0.5, \n",
    "                 ones, \n",
    "                 zeros).int() == y_test).float())\n",
    "\n",
    "print('Training Accuracy: %.2f' % (train_acc*100))\n",
    "print('Test Accuracy: %.2f' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline3(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Adaline3, self).__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, 1)\n",
    "        \n",
    "        # change random weights to zero\n",
    "        # (don't do this for multi-layer nets!)\n",
    "        self.linear.weight.detach().zero_()\n",
    "        self.linear.bias.detach().zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        netinputs = self.linear(x)\n",
    "        activations = netinputs\n",
    "        return activations.view(-1)\n",
    "\n",
    "####################################################\n",
    "##### Training and evaluation wrappers\n",
    "###################################################\n",
    "\n",
    "def train(model, x, y, num_epochs, learning_rate= 0.01, seed=123, minibatch_size=10):\n",
    "    cost = []\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        #### Shuffle epoch\n",
    "        shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n",
    "        minibatches = torch.split(shuffle_idx, minibatch_size)\n",
    "        \n",
    "        for minibatch_idx in minibatches:\n",
    "\n",
    "            #### Compute outputs ####\n",
    "            yhat = model.forward(x[minibatch_idx])\n",
    "            # yhat = model(x[minibatch_idx])\n",
    "            \n",
    "            # you could also use our \"manual\" loss_func\n",
    "            loss = F.mse_loss(yhat, y[minibatch_idx])\n",
    "            \n",
    "            #### Reset gradients from previous iteration ####\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #### Compute gradients ####\n",
    "            loss.backward()\n",
    "            \n",
    "            #### Update weights ####\n",
    "            optimizer.step()\n",
    "\n",
    "        #### Logging ####\n",
    "        with torch.no_grad():\n",
    "            # context manager to\n",
    "            # avoid building graph during \"inference\"\n",
    "            # to save memory\n",
    "            yhat = model.forward(x)\n",
    "            curr_loss = loss_func(yhat, y)\n",
    "            print('Epoch: %03d' % (e+1), end=\"\")\n",
    "            print(' | MSE: %.5f' % curr_loss)\n",
    "            cost.append(curr_loss)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0., 0.]], requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True)]\n",
      "Epoch: 001 | MSE: 0.38849\n",
      "Epoch: 002 | MSE: 0.31679\n",
      "Epoch: 003 | MSE: 0.26379\n",
      "Epoch: 004 | MSE: 0.22463\n",
      "Epoch: 005 | MSE: 0.19527\n",
      "Epoch: 006 | MSE: 0.17307\n",
      "Epoch: 007 | MSE: 0.15629\n",
      "Epoch: 008 | MSE: 0.14352\n",
      "Epoch: 009 | MSE: 0.13360\n",
      "Epoch: 010 | MSE: 0.12600\n",
      "Epoch: 011 | MSE: 0.12007\n",
      "Epoch: 012 | MSE: 0.11547\n",
      "Epoch: 013 | MSE: 0.11178\n",
      "Epoch: 014 | MSE: 0.10884\n",
      "Epoch: 015 | MSE: 0.10656\n",
      "Epoch: 016 | MSE: 0.10470\n",
      "Epoch: 017 | MSE: 0.10320\n",
      "Epoch: 018 | MSE: 0.10200\n",
      "Epoch: 019 | MSE: 0.10105\n",
      "Epoch: 020 | MSE: 0.10025\n"
     ]
    }
   ],
   "source": [
    "model = Adaline3(num_features=X_train.size(1))\n",
    "print(list(model.parameters()))\n",
    "cost = train(model, \n",
    "             X_train, y_train.float(),\n",
    "             num_epochs=20,\n",
    "             learning_rate=0.01,\n",
    "             seed=123,\n",
    "             minibatch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0189,  0.3642]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4580], requires_grad=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
